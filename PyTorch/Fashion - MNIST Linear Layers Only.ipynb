{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "torch.set_printoptions(linewidth=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=torchvision.datasets.FashionMNIST(\n",
    "root='./data/FashionMNIST'\n",
    ", train=True\n",
    ",download=True\n",
    ",transform=transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "train_loader= torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=100)\n",
    "\n",
    "images,labels=next(iter(train_loader))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape above is important because we have to understand that the linear layer expects input in SamplesXFeatures format. So we later have to convert every batch to 100 X 28*28 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Network,self).__init__()\n",
    "        #self.conv1 = nn.Conv2d(in_channels = 1 , out_channels=6, kernel_size=5)   #First layer in_channel=1 since our input has only 1 channel\n",
    "        #self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        ##Fully connectd layers below\n",
    "        self.fc1=nn.Linear(in_features=28*28, out_features=720)\n",
    "        self.fc2=nn.Linear(in_features=720, out_features=360)\n",
    "        self.out=nn.Linear(in_features=360, out_features=10)   ##Since we have 10 classes to predict \n",
    "        \n",
    "    def forward(self, t):\n",
    "            # (1) input layer\n",
    "            t = t\n",
    "\n",
    "\n",
    "\n",
    "            # (2) hidden linear layer\n",
    "            t = t.reshape(-1,28*28)   ##Converting 100 X 1 X 28 X 28 to batch_size X 28*28.  -1 here indicates figure out on your own\n",
    "            t = self.fc1(t)\n",
    "            t = F.relu(t)\n",
    "\n",
    "            # (3) hidden linear layer\n",
    "            t = self.fc2(t)\n",
    "            t = F.relu(t)\n",
    "\n",
    "            # (4) output layer\n",
    "            t = self.out(t)\n",
    "            #t = F.softmax(t, dim=1)\n",
    "\n",
    "            return t\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "network=Network()\n",
    "\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "grid = torchvision.utils.make_grid(images)\n",
    "tb = SummaryWriter()\n",
    "tb.add_image('images', grid)\n",
    "tb.add_graph(network, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 total_correct: 48280 loss: 327.86426216363907\n",
      "epoch: 1 total_correct: 51198 loss: 243.45736096799374\n",
      "epoch: 2 total_correct: 51680 loss: 230.28387935459614\n",
      "epoch: 3 total_correct: 52072 loss: 216.760657325387\n",
      "epoch: 4 total_correct: 52405 loss: 208.59409792721272\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    total_loss=0\n",
    "    total_correct=0\n",
    "    for batch in train_loader:\n",
    "        images,labels=batch\n",
    "        preds=network(images)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss= total_loss+ loss.item()\n",
    "        total_correct=total_correct+ get_num_correct(preds,labels)\n",
    "    tb.add_scalar('Loss', total_loss, epoch)\n",
    "    tb.add_scalar('Number Correct', total_correct, epoch)\n",
    "    tb.add_scalar('Accuracy', total_correct / len(train_set), epoch)\n",
    "\n",
    "    tb.add_histogram('fc1.bias', network.fc1.bias, epoch)\n",
    "    tb.add_histogram('fc2.weight', network.fc2.weight, epoch)\n",
    "    tb.add_histogram(\n",
    "        'fc1.weight.grad'\n",
    "        ,network.fc1.weight.grad\n",
    "        ,epoch\n",
    "    )\n",
    "    print(\n",
    "        \"epoch:\", epoch, \n",
    "        \"total_correct:\", total_correct, \n",
    "        \"loss:\", total_loss\n",
    "    )\n",
    "tb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
